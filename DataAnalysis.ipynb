{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import preprocessor as p\n",
    "import sys\n",
    "sys.path.append(\"/Library/Python/2.7/site-packages\")\n",
    "from wordcloud import WordCloud\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "from textblob import TextBlob   \n",
    "import nltk\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from nltk import ngrams\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(\"tweets_1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = f.readlines()\n",
    "clean_data = []\n",
    "for line in data:\n",
    "    if line[:2] == \"79\":\n",
    "        clean_data.append(line.strip())\n",
    "    else:\n",
    "        temp = clean_data[-1] + \" \" +  line.strip()\n",
    "        clean_data[-1]  = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "for tweet in clean_data:\n",
    "    try:\n",
    "        if tweet[:2] == \"79\":\n",
    "            df.append({'tweet_id': tweet[:18], 'text': tweet[19:]})\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_hashtags = {}\n",
    "def parse_tweets(tweet):\n",
    "    parsed_tweet = p.parse(tweet.decode('ascii', 'ignore').encode('ascii').lower())\n",
    "    parsed_hashtags = parsed_tweet.hashtags\n",
    "    \n",
    "    hashtags = []\n",
    "    if parsed_hashtags is not None:\n",
    "        for hashtag in parsed_hashtags:\n",
    "            temp = hashtag.match[1:].lower()\n",
    "            if temp in all_hashtags:\n",
    "                all_hashtags[temp] += 1\n",
    "            else:\n",
    "                all_hashtags[temp] = 1\n",
    "            hashtags.append(temp)\n",
    "    \n",
    "    clean_tweet = p.clean(tweet)\n",
    "    hashtags_str = (\" \").join(hashtags)\n",
    "    return clean_tweet, hashtags_str, len(hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['tweet'], df['hashtags'], df['length'] = zip(*df['text'].map(parse_tweets)) \n",
    "df = df.drop_duplicates(\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordcloud for all the secondary hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud().generate_from_frequencies(all_hashtags)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# concepts = pd.DataFrame(all_hashtags.items(), columns=['topic', 'n'])\n",
    "concept_dict = {}\n",
    "for k, d in enumerate(all_hashtags):\n",
    "    concept_dict[d] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df['text'].tolist()\n",
    "print('# of tweets:', len(tweets))\n",
    "for tweet in tweets[:5]:\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create hashtags based term document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "tweet_hashtags = df['hashtags'].tolist()\n",
    "vectorizer = TfidfVectorizer(max_features = 20000, use_idf=False)\n",
    "tf_vectors = vectorizer.fit_transform(tweet_hashtags)\n",
    "\n",
    "svd = TruncatedSVD(n_components=50, random_state=0)\n",
    "svd_tf_vectors = svd.fit_transform(tf_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load glove vectors, tokenize tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(\"loading glove model...\")\n",
    "embedding_size = 100\n",
    "glove_file = 'glove.twitter.27B.100d.txt'\n",
    "glove = {}\n",
    "with open(glove_file) as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.replace(\"\\n\",\"\").split(\" \")\n",
    "        glove[line[0]] = np.array(line[1:],dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "def tweetVector(tweet):\n",
    "    words = tknzr.tokenize(tweet.lower().replace(\"#\",\" \"))                       \n",
    "    l = float(len(tweet) | 1)\n",
    "    sum = np.zeros(embedding_size)\n",
    "    for word in tweet:\n",
    "        sum += glove.get(word, np.zeros(embedding_size))\n",
    "    return sum/l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_vectors = []\n",
    "for i in range(25000):\n",
    "    tweet_vectors.append(tweetVector(tweets[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_vectors = np.array(tweet_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Denoising AutoEncoders to create representation vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DenoisingAutoencoder import DenoisingAutoencoder\n",
    "from StackedDenoisingAutoencoders import StackedDenoisingAutoencoders\n",
    "\n",
    "da = DenoisingAutoencoder(n_hidden=400, verbose=True, training_epochs=5)\n",
    "da.fit(tweet_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_vectors = da.transform_latent_representation(tweet_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "num_clusters = 20\n",
    "kmeans_model = MiniBatchKMeans(n_clusters=num_clusters, init='k-means++', n_init=1, \n",
    "                         init_size=1000, batch_size=1000, verbose=False, max_iter=1000)\n",
    "kmeans = kmeans_model.fit(tf_vectors)\n",
    "kmeans_clusters = kmeans.predict(tf_vectors)\n",
    "kmeans_distances = kmeans.transform(tf_vectors)\n",
    "\n",
    "for i, tweet in enumerate(tweets):\n",
    "    if(i < 15):\n",
    "        print(\"Cluster \" + str(kmeans_clusters[i]) + \": \" + tweet + \"(distance: \" + str(kmeans_distances[i][kmeans_clusters[i]]) + \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating Collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_score(bigram, wfd, bfd, n_xx):\n",
    "    n_ix = wfd[bigram[0]]\n",
    "    n_xi = wfd[bigram[1]]\n",
    "    n_ii = bfd[bigram]\n",
    "    n_oi = n_xi - n_ii\n",
    "    n_io = n_ix - n_ii\n",
    "    n_oo = n_xx - n_ii - n_oi - n_io\n",
    "    score = (float(n_ii*n_oo - n_io*n_oi)**2 /\n",
    "                ((n_ii + n_io) * (n_ii + n_oi) * (n_io + n_oo) * (n_oi + n_oo)))\n",
    "    return score\n",
    "\n",
    "tknzr = nltk.TweetTokenizer()\n",
    "\n",
    "wfd = FreqDist()\n",
    "bfd = FreqDist()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "for i in tqdm(range(len(tweets))):\n",
    "    tweet = \"\".join(l for l in tweets[i] if l not in string.punctuation)\n",
    "    data_tokens = (tknzr.tokenize(tweet.lower()))\n",
    "    tokens = [w for w in data_tokens if w.lower() not in stopwords]\n",
    "\n",
    "    for window in ngrams(tokens, 2, pad_right=True):\n",
    "        w1 = window[0]\n",
    "        if w1 is None:\n",
    "            continue\n",
    "        wfd[w1] += 1\n",
    "        for w2 in window[1:]:\n",
    "            if w2 is not None:\n",
    "                bfd[(w1, w2)] += 1\n",
    "            \n",
    "after_filter_bigrams = FreqDist()\n",
    "for words, freq in bfd.iteritems():\n",
    "    if not (freq < 3):\n",
    "        after_filter_bigrams[words] = freq\n",
    "\n",
    "score_FD = FreqDist()\n",
    "for bigram in  after_filter_bigrams:\n",
    "    score_FD[bigram] = get_score(bigram, wfd, after_filter_bigrams, after_filter_bigrams.N())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_distance(w1, w2):\n",
    "    if(w1, w2) in score_FD:\n",
    "        return score_FD[(w1, w2)]\n",
    "\n",
    "words = all_hashtags.keys()\n",
    "words = np.asarray(words) #So that indexing with a list will work\n",
    "lev_similarity = -1*np.array([[compute_distance(w1,w2) for w1 in words] for w2 in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Skip vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"skip-thoughts\")\n",
    "import skipthoughts\n",
    "import numpy as np\n",
    "\n",
    "model = skipthoughts.load_model()\n",
    "encoder = skipthoughts.Encoder(model)\n",
    "skip_vectors = []\n",
    "\n",
    "for tweet in tweets:    \n",
    "    try:\n",
    "        tweet = tweet.decode('ascii', 'ignore').encode('ascii').lower()\n",
    "        skip_vectors.append(encoder.encode(tweet, verbose=0))\n",
    "    except:\n",
    "        skip_vectors.append(np.zeros(4800))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "import rcc\n",
    "clusterer = rcc.RccCluster(measure='cosine')\n",
    "\n",
    "P = clusterer.fit(svd_tf_vectors[:25000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lda\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tweets = [tweet.lower().replace(\"#\",\" \") for tweet in tweets[:25000]]\n",
    "cvectorizer = CountVectorizer(min_df=4, max_features=10000, stop_words='english')\n",
    "cvz = cvectorizer.fit_transform(tweets)\n",
    "\n",
    "n_topics = 15\n",
    "n_iter = 2000\n",
    "lda_model = lda.LDA(n_topics=n_topics, n_iter=n_iter)\n",
    "X_topics = lda_model.fit_transform(cvz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 8\n",
    "topic_summaries = []\n",
    "\n",
    "topic_word = lda_model.topic_word_  # get the topic words\n",
    "vocab = cvectorizer.get_feature_names()\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment_score(tweet):\n",
    "    ss = sid.polarity_scores(tweet)\n",
    "    return ss[\"pos\"], ss[\"neg\"], ss[\"neu\"]\n",
    "\n",
    "df['pos_score'], df['neg_score'], df['neu_score'] = zip(*df['text'].map(get_sentiment_score)) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "def language_detect(tweet):\n",
    "    return detect(tweet)\n",
    "\n",
    "df['lang'] = df[df['tweet'].str.len() > 3]['tweet'].map(language_detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
